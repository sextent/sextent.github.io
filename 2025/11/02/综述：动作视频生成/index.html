<!--
	作者：Sariay
	时间：2018-08-26
	描述：There may be a bug, but don't worry, Qiling(器灵) says that it can work normally! aha!
-->
<!DOCTYPE html>
<html class="html-loading">
		

<head>
	<meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0, user-scalable=no">
  <title>
    
      论文阅读：&#34;Human Motion Video Generation:A Survey&#34; | 六分仪的主页
    
  </title>
  <meta name="author" content="Zehao Jiang">
  <meta name="keywords" content="" />
  <meta name="description" content="" />
	<!-- favicon -->
  <link rel="shortcut icon" href="/img/favicon.ico">

  <!-- css -->
  
<link rel="stylesheet" href="/css/Annie.css">

  
  <!-- jquery -->
	
<script src="/plugin/jquery/jquery.min.js"></script>


<script>
    const CONFIG_BGIMAGE = {
      mode: 'normal',
      normalSrc: '/img/header-bg.jpg',
      randomYouMax: 110,
      randomYouSrc: 'https://sariay.github.io/Random-img/',
	  randomOtherSrc: 'https://api.berryapi.net/?service=App.Bing.Images&day=-0',
	  preloaderEnable: true
    }
	
    const CONFIG_LEACLOUD_COUNT = {
      enable: false,
	  appId: 'AU8...',
	  appKey: '4cU...',
	  serverURLs: 'http' || ' '
    }
  </script>
<meta name="generator" content="Hexo 7.3.0"></head>
	<body>
		<!-- Preloader -->

	<div id="preloader">
		<div class="pre-container">
			
				<div class="spinner">
					<div class="double-bounce1"></div>
					<div class="double-bounce2"></div>
				</div>
						
		</div>
	</div>


<!-- header -->
<header class="fixbackground bg-pan-br">
	<div class="mask">
		<!-- motto -->
		<div class="h-body">	
			
				<div class="motto text-shadow-pop-left">
					<p class="content" id="motto-content">获取中...</p>
					<p>-<p>
					<p class="author" id="motto-author">Just a minute...</p>
				</div>
			
		</div>
		
		<!-- others: such as time... -->			
		<div class="h-footer">
			<a href="javascript:;" id="read-more" class="scroll-down">
				<span class="icon-anchor1 animation-scroll-down"></span>
			</a>
		</div>
	</div>
</header>

<div id="navigation-hide">
	<!-- Progress bar -->
	<div id="progress-bar"></div>

	<!-- Progress percent -->
	<div id="progress-percentage"><span>0.0%</span></div>

	<div class="toc-switch"><span class="switch-button">目录</span></div>

	<!-- Page title -->
	<p>
		
			「论文阅读：&#34;Human Motion Video Generation:A Survey&#34;」
		
	</p>

	
	

	<!-- Nav trigger for navigation-H-->
	<a class="nav-trigger"><span></span></a>
</div>

<!-- Navigation in div(id="navigation-H") -->
<nav class="nav-container" id="cd-nav">
	<div class="nav-header">
		<span class="logo"> 
			<img src="/img/logo.png">
		</span>
		<a href="javascript:;" class="nav-close"></a>
	</div>
	
	<div class="nav-body">
		<ul id="global-nav">
	
		<li class="menu-home">
			<a href="/" class="menu-item-home" target="_blank">主页</a>
		</li>
		
	
		<li class="menu-archive">
			<a href="/archives" class="menu-item-archive" target="_blank">归档</a>
		</li>
		
	
		<li class="menu-categories">
			<a href="/categories" class="menu-item-categories" target="_blank">分类</a>
		</li>
		
	
		<li class="menu-tags">
			<a href="/tags" class="menu-item-tags" target="_blank">标签</a>
		</li>
		
	
		<li class="menu-about">
			<a href="/about" class="menu-item-about" target="_blank">关于</a>
		</li>
		
	
		<li class="menu-gallery">
			<a href="/gallery" class="menu-item-gallery" target="_blank">相册</a>
		</li>
		
	

	
		<li class="menu-search">
			<a href="javascript:;" class="popup-trigger">搜索</a>
		</li>
	
</ul>
	</div>
	
	<div class="nav-footer">
		<ul id="global-social">
	
		<li>
			<a href="http://github.com/" target="_blank">
				<span class="icon-one"><span class="path1"></span><span class="path2"></span></span>
			</a>
		</li>
	
		<li>
			<a href="http://github.com/" target="_blank">
				<span class="icon-zhihu"></span>
			</a>
		</li>
	
		<li>
			<a href="http://github.com/sextent" target="_blank">
				<span class="icon-github"></span>
			</a>
		</li>
	
		<li>
			<a href="http://github.com/" target="_blank">
				<span class="icon-sina-weibo "></span>
			</a>
		</li>
	
		<li>
			<a href="http://github.com/" target="_blank">
				<span class="icon-pinterest2"></span>
			</a>
		</li>
	
		<li>
			<a href="http://github.com/" target="_blank">
				<span class="icon-instagram"></span>
			</a>
		</li>
	
		<li>
			<a href="http://github.com/" target="_blank">
				<span class="icon-twitter"></span>
			</a>
		</li>
	
		<li>
			<a href="/atom.xml" target="_blank">
				<span class="icon-rss"></span>
			</a>
		</li>
			
</ul>

	</div>
</nav>
			
		<!--main-->
		<main>
			<!--
	时间：2018-11-17
	描述：
		插件名称：katelog.min.js
		插件作者：KELEN
		插件来源: https://github.com/KELEN/katelog
-->

	
		<div class="layout-toc">
			<div id="layout-toc">
				<div class="k-catelog-list" id="catelog-list" data-title="文章目录"></div>
			</div>
		</div>

		
<script src="/plugin/toc/katelog.min.js"></script>


		
	 

<div class="layout-post">
	<div id="layout-post">
		<div class="article-title">
			
	<a href="/2025/11/02/%E7%BB%BC%E8%BF%B0%EF%BC%9A%E5%8A%A8%E4%BD%9C%E8%A7%86%E9%A2%91%E7%94%9F%E6%88%90/" itemprop="url">
		论文阅读：&#34;Human Motion Video Generation:A Survey&#34;
	</a>

		</div>

		<div class="article-meta">
			<span>
				<i class="icon-calendar1"></i>
				
				




	更新于

	<a href="/2025/11/02/%E7%BB%BC%E8%BF%B0%EF%BC%9A%E5%8A%A8%E4%BD%9C%E8%A7%86%E9%A2%91%E7%94%9F%E6%88%90/" itemprop="url">
		<time datetime="2025-11-02T06:50:47.000Z" itemprop="dateUpdated">
	  		2025-11-11
	  </time>
	</a> 



			</span>
			<span>
				
	<i class="icon-price-tags"></i>
	
		<a href="/tags/%E7%AC%94%E8%AE%B0/" class=" ">
			笔记
		</a>
	
		
			</span>
			
			



		</div>

		<div class="article-content" id="article-content">
			<p>本文主要解读发表在IEEE TPAMI（模式识别与机器智能汇刊）上的综述：《Human Motion Video Generation: A Survey》。</p>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><ul>
<li><strong>论文标题</strong>  Human Motion Video Generation: A Survey</li>
<li><strong>期刊</strong> IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2025</li>
<li><strong>论文地址</strong> <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.03883">https://arxiv.org/pdf/2509.03883</a></li>
<li><strong>项目地址</strong> <a target="_blank" rel="noopener" href="https://github.com/Winn1y/Awesome-Human-Motion-Video-Generation">https://github.com/Winn1y/Awesome-Human-Motion-Video-Generation</a></li>
</ul>
<p>人类动作视频生成因其广泛的应用而引起了重要的研究关注,使得诸如逼真的歌唱头像或随音乐无缝舞动的动态虚拟形象等创新成为可能。然而,该领域现有的综述主要关注单个方法,缺乏对整个生成过程的全面概述。本文通过提供人类动作视频生成的深入综述来填补这一空白,涵盖了十多个子任务,并详细阐述了生成过程的<strong>五个关键阶段</strong>:输入、动作规划、动作视频生成、优化和输出。值得注意的是,这是首次讨论大语言模型在增强人类动作视频生成方面潜力的综述。我们的综述回顾了人类动作视频生成在<strong>三种主要模态</strong>(视觉、文本和音频)方面的最新发展和技术趋势。通过涵盖两百多篇论文,我们提供了该领域的全面概述,并重点介绍了推动重大技术突破的里程碑式工作。本综述的目标是揭示人类动作视频生成的前景,并作为推进数字人类综合应用的宝贵资源。</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>人类动作视频生成是指创建描绘逼真或风格化人类动作,这些动作基于各种输入,如视觉线索、文本提示和音频信号。该领域的早期工作仅限于创建卡通角色或缺乏逼真纹理的人体模型[1], [2]。随着通用文本到视频生成模型的出现[3]–[5],该领域已经扩展到能够生成具有逼真纹理和类人质量的视频。<br>近年来,以人为中心的视频合成研究[6], [7]蓬勃发展,在说话人头像、肖像动画和舞蹈视频生成等领域获得了极大关注,如图1所示。为了最小化恐怖谷效应并增强人机交互,逼真人类动作视频的生成已成为一个突出的研究主题,涉及创建具有类人外观、逼真动作和自然表情的视频。<br><img src="/images/2025-11-3-1.png"><br>现有综述[10]–[14]通常集中于人类动作视频生成中的特定子任务,并未提供生成以人为中心视频的完整流程。因此,我们明确定义了人类动作视频生成任务的五个关键阶段,如图2所示。这些阶段协同工作,将参考输入转换为高保真、逼真的人类动作视频,实现实时且成本高效的应用。该过程始于识别驱动源,包括视觉线索、文本提示或音频信号。值得注意的是,生成面部区域和整体人体通常需要不同的框架。第二阶段涉及基于这些输入的动作规划。虽然以前的方法[15], [16]使用输入条件来设计人类动作的隐式特征映射,但最近的研究[17]–[19]探索使用大语言模型(LLMs)进行动作规划。第三阶段专注于生成人类动作视频,确保身体一致性、精确的动作和高质量输出。第四阶段涉及增强生成的视频,优化手部动作,同步口腔和牙齿运动,调整眼神,并提高整体视频质量。最后,第五阶段解决数字人类在实时流媒体平台上的部署问题,集成实用功能。<br><img src="/images/2025-11-3-2.jpg"><br>我们确定了驱动人类动作视频生成的三种主要模态:视觉、文本和音频。鉴于许多方法涉及多种模态,其中视觉是普遍存在的，且当两者都存在时音频比文本具有更强的影响力,我们基于以下标准对方法进行分类：</p>
<ul>
<li>音频驱动：（音频+文本+视觉或音频+视觉）。如果一个方法包含了音频，那么它就被归类为音频驱动的，即使有其他模式（modalities）存在。</li>
<li>文本驱动（文本+视觉）。如果一个方法不属于音频驱动，但是包含了文本，那么它就可以被分类为文本驱动的。</li>
<li>视觉驱动（只有vision）。如果只有视觉模式，那么分类为视觉驱动方法。<br>这个分类系统有效地将200多篇论文组织到这三个不同的类别中，这三种驱动模态的代表性工作时间线如图3所示。<br><img src="/images/2025-11-3-3.png"><br>对于基于视觉的条件，肖像动画主要专注于生成特定的面部表情。在更广泛的范围内，姿态驱动、视频驱动的舞蹈视频生成和虚拟试衣技术代表了新兴的研究热点。需要注意的是，我们强调肖像动画和说话人头像之间的区别。肖像动画依赖于参考图像和姿态序列，而说话人头像利用参考图像和驱动音频，通常被称为音频驱动的肖像动画。文本驱动方法包括Text2Face，它从第一人称脚本或指令生成面部动画，以及Text2MotionVideo，它将这一概念扩展到从文本提示创建整体人类动作。虽然当前的大部分研究集中在Text2Motion3D上，正如Zhu等人[20]所讨论的，我们的综述专门针对人类动作视频生成，将3D骨骼动作排除在其范围之外。在音频驱动的场景中，我们按照驱动区域从小到大的顺序详细阐述相关研究，包括音频-唇形同步、头部姿态驱动和整体人体动作生成。值得注意的是，本文讨论的方法利用多模态条件输入，其中生成持续时间由驱动动作序列决定。<br><img src="/images/2025-11-4.png"><br>与表I中列出的以往综述[10], [11]不同，我们的工作提供了人类动作视频生成中五个关键阶段的全面定义。值得注意的是，我们是首次讨论大语言模型在动作规划中的应用和潜力的。我们的综述涵盖了更广泛的范围，提供了各种方法的详细分类。在这个框架内，出现了各种子分支，每个分支都专注于人类动作视频生成的特定方面，如表II所总结的。此外，我们收集了64个以人为中心的数据集来支持相关任务，提供了诸如数据持续时间和分辨率等详细信息。而且，我们识别了当前的挑战，同时为未来的研究和发展方向提供了见解。本文的主要贡献总结如下：</li>
<li>我们将人类动作视频生成分解为五个关键阶段，涵盖了各种驱动源和身体区域的所有子任务。据我们所知，这是首次为人类动作视频生成提供如此全面框架的综述。</li>
<li>我们从动作规划和动作生成两个角度对人类动作视频生成进行了深入分析，这一维度在现有综述中尚未得到充分探索。</li>
<li>我们清晰地界定了已建立的基准和评估指标，为塑造该领域的关键挑战提供了详细的见解。</li>
<li>我们提出了一系列潜在的未来研究方向，旨在启发和指导人类动作视频生成领域的研究人员。</li>
</ul>
<p>鉴于人类动作视频生成的重大进展和广泛应用，我们提供了一个全面的综述来帮助学术界跟踪其进展。本综述的其余部分组织如下：第二节建立了理解人类动作视频生成所必需的基础知识。接下来，第三节介绍了利用大语言模型进行人类动作规划的创新方法。随后，第四节深入探讨了动作建模和视频生成的方法论。</p>
<p>在第五节中，我们简要探讨了最后两个阶段的策略：优化和输出。在第六节中，我们整合了流行的指标和数据集。最后，第七节讨论了当前的挑战，并为人类动作视频生成提供了潜在的未来方向。请注意，本文中引用的所有统计数据截至2024年8月30日。</p>
<h2 id="Preliminary"><a href="#Preliminary" class="headerlink" title="Preliminary"></a>Preliminary</h2><p>这一节主要介绍理解人类动作视频生成所必需的基础知识。</p>
<h3 id="A-Generate-Framework"><a href="#A-Generate-Framework" class="headerlink" title="A.Generate Framework"></a>A.Generate Framework</h3><p><strong>变分自编码器(VAE)</strong>。VAE由Kingma和Welling[57]于2013年提出，已成为一个突出的生成模型，以其强大的数据表示能力而闻名。在人类动作视频生成任务中，VAE及其变体在编码视觉信号或参考图像方面发挥重要作用，促进相应视频的生成[58], [59]。这些因素导致了变体的采用，如VQ-VAE[60]。然而，VAE容易受到模式坍塌的影响，生成的样本通常不如生成对抗网络生成的样本清晰。因此，VAE与扩散模型结合以提高生成输出质量。<br><strong>生成对抗网络(GANs)</strong>。GANs由Ian Goodfellow等人于2014年首次提出[61]，由两个对抗神经网络组成：生成器G和判别器D。生成器创建模仿真实样本的数据，而判别器区分真实数据和生成数据。与VAE中强数学先验导致的生成质量下降不同，GANs隐式映射特征关系，产生更高质量的生成输出。StyleGAN[62], [63]等变体取得了重要里程碑，特别是在人类动作视频生成方面，包括动作复制[29], [64]和说话人头像合成[6], [41]等任务。然而，GANs受到生成样本多样性不足的限制，并且容易出现模式坍塌，这主要是由于在平衡生成器和判别器之间的训练动态方面存在挑战。<br><strong>扩散模型(DMs)</strong>。扩散模型[65]吸引了生成建模领域的广泛关注[66]–[70]。这些模型通过逐步对初始噪声输入进行去噪来生成样本，其训练目标可以表示为重新加权的变分下界[71]，提供了分布覆盖、稳定的训练目标和易于扩展性等优势[69]。</p>
<p>然而，尽管实现了高质量的样本生成，扩散模型会产生显著的计算开销。潜在扩散模型(LDMs)在潜在空间中应用扩散过程，显著降低了计算成本。例如，稳定视频扩散(SVD)[5]和Animatediff[4]是采用LDMs的经典文本到视频生成方法。人类动作视频生成的几项研究[7], [72]基于这种方法，将文本驱动模型扩展为多条件驱动模型。</p>
<h3 id="B-人体数据表示"><a href="#B-人体数据表示" class="headerlink" title="B.人体数据表示"></a>B.人体数据表示</h3><p>根据Knap [73]的分类，我们将人体姿态表示分为七种关键类型，如图4所示。</p>
<p><strong>掩码（Mask）</strong>。掩码勾勒出角色的基本轮廓和占用区域，提供粗粒度的布局先验[72]。然而，它们缺乏详细的姿态描述。</p>
<p><strong>网格（Mesh）</strong>。网格提供了更详细的身体形状表示，包括肢体弯曲和遮挡。目前，基于网格的表示被应用于克服关键点估计中3D空间信息的丢失[74]。</p>
<p><strong>深度（Depth）</strong>。对空间关系理解不足限制了现有方法准确生成被遮挡身体部位的能力[34]。融入深度线索可以有效利用空间信息，帮助模型学习角色之间的空间关系。</p>
<p><strong>法线（Normal）</strong>。法线条件关键性地强调了人体的方向，确保在生成逼真人体姿态时的精确对齐，以增强视觉保真度[74]。这种对齐对于维持动画中空间关系的完整性至关重要，从而显著提高渲染场景的可信度和沉浸感质量。</p>
<p><strong>关键点（Keypoint）</strong>。现有的姿态关键点估计方法通常采用OpenPose [75]或DWPose [76]等模型。最近的研究[33]、[77]使用DWPose [76]作为基础的基于关键点的人体骨架表示。</p>
<p><strong>语义（Semantics）</strong>。语义条件优先考虑不同身体部位的信息形成，促进不同特征的解耦。这种方法能够对每个身体部位进行有针对性的增强和独立操作，从而改进以人为中心的模型中的表示和交互动态[78]。</p>
<p><strong>光流（Optical Flow）</strong>。直接在噪声数据集上优化模型往往会导致背景不稳定。Follow Your Pose2 [34]通过添加额外的光流条件来减少噪声数据的影响，使模型能够适应背景中的不稳定变化。<br><img src="/images/2025-11-3-5.png"></p>
<h2 id="人体运动规划"><a href="#人体运动规划" class="headerlink" title="人体运动规划"></a>人体运动规划</h2><p>人体运动规划阶段对于确定虚拟数字人执行的具体动作至关重要。从输入信号生成的运动序列使虚拟角色能够展现高度自然的动作，符合人类习惯，并与周围物体平滑交互，有效地模拟人类行为。</p>
<p>目前，人体运动规划主要由两种方法驱动：一种是利用大语言模型（LLMs）的力量进行运动规划，另一种是依赖不同特征的映射来生成运动。本节强调了大语言模型在人体运动视频生成中日益增长的重要性。通过利用大语言模型中嵌入的固有先验知识，这些模型能够更好地理解语义细节并推理情感。</p>
<h3 id="A-使用大语言模型的运动规划"><a href="#A-使用大语言模型的运动规划" class="headerlink" title="A. 使用大语言模型的运动规划"></a>A. 使用大语言模型的运动规划</h3><p>在人体运动视频生成领域，Geng等人[17]、AgentAvatar [18]和InstructAvatar [19]的研究代表了将大语言模型应用于运动规划的当前进展。值得注意的是，Geng等人[17]开创性地使用大语言模型分析两个人在对话中的对话特征，从而推断出听者的适当表情。另外两项研究专注于单人人体运动视频的生成。<br>作为该领域的开创性工作之一，Geng等人[17]展示了大语言模型在人体运动视频生成中的创新应用。他们的方法首先将说话者的台词和对话意图输入大语言模型，然后生成听者的合理反应，如微妙的微笑。这些生成的运动描述随后用于训练CLIP模块[79]，有效地将大语言模型与运动生成相结合。</p>
<p>AgentAvatar [18]采用更广泛的方法，将大语言模型融入人体运动视频生成的一般语境中。这种方法超越了对话驱动的场景，能够在广泛的语境中规划和生成人体运动。他们的过程从环境概览和基于大语言模型规划器的化身设置开始。规划器生成面部运动的详细描述，然后传递给驱动引擎以产生逼真的视频序列。</p>
<p>InstructAvatar [19]设计了一个自动标注流水线来构建丰富的指令-视频对数据集。该数据集使用动作单元（AUs）来描述面部肌肉运动，从而捕获细粒度的面部细节。动作单元通过现成的模型提取，并通过多模态大语言模型进行精炼，将动作单元转述为自然的文本描述。这个过程不仅用详细的情感和运动描述丰富了数据集，还增强了模型的泛化能力。<br>在各种表情和情感中的泛化能力。如图6所示，InstructAvatar [19]的架构基于VAE [57]，它将运动从外观中解耦，允许对这些元素进行单独和集中的操作。基于Conformer架构[80]的运动生成器采用扩散模型来学习从音频和文本指令到运动潜在空间的映射。该模型进一步配备了双分支交叉注意力机制，能够区分情感和运动指令，确保化身在整个视频中保持一致的情感状态，同时按照文本指示执行动态面部运动。</p>
<p>这些研究[17]–[19]揭示了大语言模型使用的两种不同的运动规划策略，如图5所示。在图5(A)中，大语言模型处理运动描述以生成细粒度的运动描述，用于在视频数据库上执行检索以获得相关片段。这种基于检索的方法被Geng等人[17]和AgentAvatar [18]等方法采用。相比之下，图5(B)展示了一种生成策略，其中派生的运动条件作为生成模型的输入，如InstructAvatar [19]所演示的。</p>
<p>大语言模型在3D骨骼运动生成中开始显示出有希望的结果，在运动规划阶段充当中央协调器。例如，FineMoGen [84]、PROMotion [85]、AvatarGPT [86]和MotionGPT [87]能够从文本生成整体人体运动。MotionScript [88]促进了运动到描述性运动脚本的无缝转换，反之亦然，建立了一个强大的双向生成机制。此外，Ng等人[89]引入了一种创新方法，利用大语言模型合成针对观众特征定制的3D面部表情。此外，InterControl [90]和MoMatMoGen [91]开始探索大语言模型在促进双向交互任务中的潜力。然而，这些努力仅限于3D骨骼运动，迄今为止尚未扩展到真实视频内容的创建。人体运动视频生成领域仍有进一步发展的空间。</p>
<p>目前，人体运动视频的生成主要依赖文本作为中介来整合大语言模型与生成模型。所有这些工作都在附录A中进行了总结。然而，对于更有效和新颖的中间表示的探索仍然缺乏。从初步分析来看，出现了不同的挑战：</p>
<ul>
<li>如表II所示，大多数现有工作[26]、[30]、[36]、[44]、[50]主要专注于学习驱动条件和运动序列之间的隐式关系。尽管大语言模型具有将驱动条件分解为细粒度特征以增强运动规划的潜力，但它们作为运动规划器仍未得到充分利用。</li>
<li>Ng等人[89]开创性地探索了超越文本描述的基于码本的中间表示，尽管基于码本的控制在运动操作中显示出有限的粒度。研究有效的运动规划中间表示仍然是一个关键的研究方向，以使大语言模型能够理解和规划行为。</li>
</ul>
<h3 id="B-通过映射特征进行运动规划-mapping"><a href="#B-通过映射特征进行运动规划-mapping" class="headerlink" title="B.通过映射特征进行运动规划(mapping)"></a>B.通过映射特征进行运动规划(mapping)</h3><p>在人体运动视频生成中，视频合成的复杂性导致大多数研究[92]–[94]采用输入特征的隐式表示。这些方法专注于学习输入条件和运动之间的映射，通常通过引入一定程度的随机噪声来生成多样化的结果。这些方法的详细讨论将在第四节中提供。</p>
<h2 id="运动模型和视频生成"><a href="#运动模型和视频生成" class="headerlink" title="运动模型和视频生成"></a>运动模型和视频生成</h2><p>在人体运动视频生成阶段，我们的目标是合成逼真的视频来捕捉整体人体运动，这基于运动规划阶段的结果。为了全面检验各种生成方法，我们对每个特定子任务进行了综合分析。本节专注于利用扩散模型的视频生成方法，排除了NeRF [8]和3DGS [9]等其他技术。我们深入研究三个关键领域：视觉引导（包括肖像动画、舞蹈视频生成、试穿和姿态到视频）、文本引导（涵盖Text2Face和Text2MotionVideo）以及音频驱动场景（如说话人头部生成和音频驱动的整体人体运动生成）。</p>
<h3 id="A-视觉驱动的人体运动视频生成"><a href="#A-视觉驱动的人体运动视频生成" class="headerlink" title="A. 视觉驱动的人体运动视频生成"></a>A. 视觉驱动的人体运动视频生成</h3><p><strong>肖像动画</strong>。肖像动画专注于使用先进技术为静态图像（通常是肖像）注入生命力。<br>动画技术。该过程从静态图像开始，无论是照片还是数字绘画，将其转换为传达主体情感表达的动画序列。该领域的最新进展总结在表III中。<br>OmniAvatar [81]展示了使用几何先验来指导动画过程，确保3D一致性和详细的面部表情。Follow-Your-Emoji [25]利用基于扩散的框架，使用目标地标序列对肖像进行动画处理。通过使用表情感知地标来指导动画，它确保了运动对齐和身份保持，同时增强了夸张表情的描绘。此外，它采用细粒度面部损失函数来改善模型对细微表情的感知和参考肖像外观的重建。LivePortrait [26]引入了一个高效的视频驱动框架，在计算效率和可控性之间取得平衡，实现快速生成速度。<br>总体而言，肖像动画只需要一张参考图像就能根据输入条件驱动面部表情，因此吸引了大量研究关注。例如，EDTN [23]和X-Portrait [27]解决了跨域头部重现的挑战，允许将人体运动转移到非人类域，如动漫角色。MobilePortrait [28]专注于实时性能，为可控渲染的说话人脸化身提供一次性解决方案。Face-Off [83]提出了一种新颖的视频到视频人脸交换系统，在适应目标视频姿态和背景的同时保持源表情和身份。从众多研究中，我们可以识别出肖像动画中的不同关键挑战：</p>
<ul>
<li>当前的面部驱动技术[25]、[27]、[81]–[83]涵盖多种面部属性，包括眼神注视、牙齿、唇部同步和头部姿态。然而，现有研究中对眼神注视一致性和牙齿细节的精确控制仍然解决不足。</li>
<li>现有方法通常需要特定身份的微调来实现最佳性能[23]、[25]、[81]，而在零样本场景中保持身份一致性是一个重大挑战。</li>
<li>大多数当前研究[25]、[26]、[28]、[81]专注于单人面部驱动，使得多人面部驱动方法的探索成为一个有前景的研究方向。</li>
</ul>
<h3 id="C-音频驱动的人体动作视频生成"><a href="#C-音频驱动的人体动作视频生成" class="headerlink" title="C.音频驱动的人体动作视频生成"></a>C.音频驱动的人体动作视频生成</h3><p>本节专注于音频驱动环境下的人体运动生成。通过输入音频和参考信号（例如参考图像或视频），我们推导出与音频信号相对应的人体运动视频。从音频建模人体运动涉及解决几个重大研究挑战，如准确捕捉唇部运动、头部姿态、音频驱动的整体人体运动以及产生细粒度动画。此外，我们研究并总结了人体运动视频生成中使用音频的范式，并为语音信号的使用提供了清晰的指导，如图10所示。<br><img src="/images/2025-11-3-10.png"><br><strong>唇部同步</strong>。早期研究主要专注于唇部运动。Wen等人[132]将音频映射到面部表情，并通过区域掩码重新渲染合成视频。Wav2lip [136]将给定音频与视频中角色的唇部运动同步，实现逼真的唇部同步效果。Chen等人[133]采用级联GANs方法来建模语音和唇部运动之间的同步，有效地将语音中与内容相关的表示与非内容相关的信号解耦，从而增强对不同面部形状和视角的鲁棒性。<br>此外，Cudeiro等人[134]提出通过3D模型结构进行唇部同步的显式建模。<br>头部姿态驱动。仅关注唇部运动无法产生说话者的逼真视觉效果。Greenwood等人[152]是使用Bi-LSTM模型[153]从音频预测头部姿态运动的先驱者。Zhou等人[53]通过面部地标显式建模说话者的<strong>头部运动</strong>.<br>训练Transformer [154]通过对抗学习捕捉长期依赖关系，从而生成自然的头部姿态。<br>Lu等人[54]提出了一个基于历史头部姿态和语音表示条件的自回归概率模型，用于预测当前时刻的运动分布。关键头部姿态从这个预测的概率模型中采样。他们的方法专注于实时音频驱动的头部姿态视频生成，能够以每秒30帧的速度产生头部姿态视频。由于音频本身不提供头部姿态和全局头部运动的线索，直接从音频推断头部运动可能导致头部姿态与目标视频中编辑头部的运动之间出现显著不匹配。因此，Ji等人[155]提出了一种目标自适应面部合成技术，在3D空间中弥合推断地标与目标视频肖像之间的姿态差距。通过采用精心设计的3D感知关键点对齐算法，2D地标可以准确投影到目标视频上。<br>为了获得更多的可控性，最近的研究[52]、[137]、[171]、[172]提出了一个两阶段框架：音频到地标和地标到视频。在第一阶段，使用wav2vec [173]提取音频特征并将其转换为2D面部地标序列。在第二阶段，扩散模型和运动模块将这些序列转换为时间一致且逼真的肖像动画。类似的两阶段方法[174]、[175]被用于改进面部表情预测并实现高保真面部渲染。此外，Meng等人[13]详细总结了关于说话头像任务的最新进展。为了提供更广阔的视角，表V提供了头部姿态驱动领域各种工作的全面概述。<br><strong>音频驱动的整体人体驱动</strong>。以往的音频驱动人体运动视频生成方法通常专注于面部表情和唇部同步，忽略了与音频同步的头部、上半身和手部运动的生成。<br>这种局限性降低了这些视频在传达更丰富人类交流方面的有效性，因为缺乏肢体语言和手势使得合成视频显得不够自然和逼真。Vlogger [55]通过基于单张输入图像和音频样本生成逼真且时间连贯的视频来解决这一差距。它采用基于Transformer的网络，输入梅尔频谱图来预测一系列3D面部表情和身体姿态参数，捕捉音频信号与人体运动之间的复杂映射关系。这种方法不仅包括头部运动、凝视和唇部动作，还生成上半身和手部手势，从而将音频驱动的合成技术推进到一个新水平。<br>类似地，一些工作专注于仅使用语音提供运动信息的整体运动视频生成。ANGIE [170]采用统一框架来生成由语音音频驱动的说话者图像序列。关键洞察在于共语手势可以分解为常见的运动模式和微妙的节奏动态。具体而言，Dance Any Beat [56]开创了音乐驱动舞蹈视频生成的任务。他们的方法基于LFDM [176]框架，仅使用一张人物照片就能从音乐中生成相应的舞蹈动作。<br><strong>细粒度风格和情感驱动动画</strong>。基于从音频中提取的情感来控制面部动作面临的主要挑战是情感提取，因为情感信息与语音内容等其他因素错综复杂地纠缠在一起。Ji等人[155]通过提出音频的交叉重构情感解耦技术来解决这个问题，该技术提取两个独立的潜在空间：一个持续时间无关的空间，编码不考虑内容的情感；一个持续时间相关的空间，编码音频的语音内容。<br>此外，DreamTalk [158]不仅管理风格化面部动画的生成，还通过风格感知唇部专家和风格预测器实现对面部表情的细粒度控制。最近，情感和语音经常同时用作条件[94]、[160]、[163]、[164]、[177]，为生成人体运动视频提供隐式运动表示。为了解决由于音频到面部表情映射的模糊性而导致的音频与扩散模型集成的复杂性，EMO [50]引入了稳定控制机制，包括速度控制器和面部区域控制器，以增强生成稳定性。此外，VASA-1 [167]集成了几个额外信号，使生成建模更易管理并改善生成过程的可控性。<br>表VI提供了这些创新方法的全面概述。确实，解决音频驱动人体运动视频生成的复杂性面临以下挑战：</p>
<ul>
<li>最近的研究在唇部同步[92]、[135]和头部运动合成[49]、[92]、[137]、[140]方面取得了进展，并融入了情感控制[50]、[51]、[165]。然而，一个主要挑战仍然存在：开发一个统一框架，将唇部同步、音频驱动的头部运动合成和节奏性手势生成[55]整合在一起，同时确保背景稳定并保持整体身体细节。</li>
<li>基于扩散的方法[51]、[158]、[159]、[165]在实现实时性能方面面临重大挑战，特别是对于音频驱动的整体人体运动视频生成。虽然轻量级神经架构[143]、[178]提供了潜在解决方案，但在扩散模型中实现低延迟推理仍然是一个关键的研究重点。<br><strong>多语言视频配音</strong>。多语言视频配音是人体运动视频生成中另一个引人入胜的任务，即将视频从一种语言翻译成另一种语言。源语言中的语音内容被转录为文本，经过翻译，然后自动合成为目标语言的语音，同时保留原始说话者的声音。视觉内容通过合成说话者的唇部运动与翻译后的音频对齐，在目标语言中创造无缝的视听体验。Yang等人[179]是最早解决这一任务的研究者之一。Bigioi等人[14]进一步阐述了该领域的挑战，强调了实现真实性、跨语言适应性以及克服数据多样性和泛化局限性的复杂性，这些问题继续推动着研究和开发工作。</li>
</ul>
<h2 id="精细化（refinement）和输出"><a href="#精细化（refinement）和输出" class="headerlink" title="精细化（refinement）和输出"></a>精细化（refinement）和输出</h2><p>人体运动视频生成领域最近因其广泛的潜在应用而受到了相当大的关注。然而，当前的生成框架仍处于初期阶段，表现出有限的控制能力。为了应对这些挑战，引入了先进的精细化策略。本节详细介绍了精细化阶段（步骤4）和输出阶段（步骤5）。</p>
<h3 id="A-精细化"><a href="#A-精细化" class="headerlink" title="A. 精细化"></a>A. 精细化</h3><p>精细化过程对于增强生成模型的输出至关重要，可以大致分为两类：特定部位精细化和通用精细化。特定部位精细化针对特定区域，而通用精细化旨在提升整体视频质量。<br>特定部位精细化涉及对特定身体部位的有针对性增强，如嘴部、眼部、牙齿和手部，这些部位通常受到生成错误的影响。这种精细化对于解决生成模型在这些精细区域的局限性至关重要。作为这一更广泛策略的一部分，有针对性的修复方法被用来纠正这些特定区域的不准确性。这些方法[22]、[26]包括开发专门的损失函数，在训练阶段针对目标区域进行定制，以及应用预训练网络进行后处理改进。例如，MimicMotion [36]展示了先进姿态引导机制的使用，并设计了手部区域增强方法来提高细微运动的精度，最小化失真，并改善整体运动保真度。此外，后处理流水线通常集成诸如Codeformer [180]或Feng等人[181]的方法等工具，以消除输出中的面部伪影。<br>通用精细化技术包括超分辨率、帧率增强和去噪网络。这些技术协同工作，增强视频的分辨率、帧率和整体清晰度，从而显著改善观看质量。</p>
<h3 id="B-输出"><a href="#B-输出" class="headerlink" title="B. 输出"></a>B. 输出</h3><p>人体运动视频的实时生成仍然是一个具有挑战性且相对未被充分探索的领域。虽然基于GAN的方法，如Guo等人[26]和Jiang等人[28]开发的方法，在说话头像和肖像动画等应用中展现出一定的实时性能能力，但它们面临训练不稳定和视频质量较低的问题。由于扩散模型在生成高质量视频方面的卓越能力，对其的关注度有所增加。然而，训练和推理的高计算成本为实时应用带来了重大挑战。尽管这些模型前景广阔，但成本优化和推理加速的研究仍处于初期阶段，这对其实际的实时应用构成了相当大的障碍。<br>新兴工作如Kodaira等人[182]和Liang等人[183]探索了使用基于流的扩散的创新实时视频编辑技术，为将这些方法与人体运动视频生成相结合提供了有前景的方向。如Sauer等人[184]和Zhai等人[185]工作中所见的模型蒸馏方面的进一步进展，预计将显著提升模型采样速度，为在不久的将来实现实时视频生成能力铺平道路。</p>
<h2 id="评估"><a href="#评估" class="headerlink" title="评估"></a>评估</h2><h3 id="A-评估指标"><a href="#A-评估指标" class="headerlink" title="A.评估指标"></a>A.评估指标</h3><p>适当的评估指标对于比较不同方法和推进该领域的发展至关重要。然而，生成的人体运动视频仍然缺乏统一的评估系统。在本节中，我们总结了常用评估指标的不同方面（图11）。<br><img src="/images/2025-11-3-11.png"></p>
<h4 id="1-单帧图像质量"><a href="#1-单帧图像质量" class="headerlink" title="1.单帧图像质量"></a>1.单帧图像质量</h4><p>在视频生成任务中，测量每个生成帧质量的目标是确保视频不仅整体上看起来连贯流畅，而且每一帧都保持高质量。常见指标包括L1距离、弗雷歇特起始距离（FID）[186]、[187]、结构相似性指数测量（SSIM）[188]、峰值信噪比（PSNR）[189]和学习感知图像块相似性（LPIPS）[190]。其他指标包括：CLIP-S [79]、面部相似性[96]、CPBD [191]、[192]和NIQE [193]。</p>
<h4 id="2-视频质量评估"><a href="#2-视频质量评估" class="headerlink" title="2.视频质量评估"></a>2.视频质量评估</h4><p>评估检查时间一致性、运动连贯性和整体视觉吸引力。关键指标包括：KVD [194]、FVD [194]、FGD [195]和ACD [196]。</p>
<h4 id="3-视频特征评估"><a href="#3-视频特征评估" class="headerlink" title="3.视频特征评估"></a>3.视频特征评估</h4><p>帧一致性CLIP分数[197]、IS [198]、多样性[199]、BCS [200]和姿态准确性[201]–[203]。</p>
<h4 id="4-视频评估器和基准测试"><a href="#4-视频评估器和基准测试" class="headerlink" title="4.视频评估器和基准测试"></a>4.视频评估器和基准测试</h4><p>用于评估生成视频质量的综合工具，包括DOVER [204]、VBench [205]和EvalCrafter [206]。尽管综合视频评估器和基准测试提供了系统化框架，但由于复杂评估框架的实现复杂性和计算成本，研究人员主要依赖传统指标（如SSIM [188]和FID-VID [186]、[187]）。</p>
<h2 id="挑战与展望"><a href="#挑战与展望" class="headerlink" title="挑战与展望"></a>挑战与展望</h2><p>人体运动视频生成面临多项挑战，包括数据可用性、信号理解、运动规划以及生成视频的质量，如图13所示。<br><strong>数据不足</strong>。 人体运动视频生成领域受到数据可用性有限的阻碍，这主要由于隐私担忧、质量差和收集成本高等问题。这种稀缺性削弱了模型的鲁棒性，并影响了现实世界的可靠性。扩展数据集对于训练模型更好地识别、理解和复制人类行为至关重要，最终提高生成视频的质量和多样性。<br><strong>运动规划器</strong>。 当前的运动规划严重依赖于预先存在的数据分布，这限制了其掌握人类动作更深层语义层面的能力。这种方法约束了适应性和复杂性。为了推进运动规划，我们必须从纯统计方法转向那些融入意义、上下文和意图的方法。利用LLM可以通过分析复杂的人体运动模式来增强这一过程，使视频中生成更真实和上下文适当的人体运动成为可能，这被视为一个有前景的方向[17]–[19]。<br><strong>缺乏真实感</strong>。 在人体运动视频生成中，如图13所示的众多挑战需要复杂的解决方案。生成的人体形态的保真度，特别是在面部和手部方面，对于真实感和表现力至关重要。<br>在整个视频中保持视觉一致性是另一个挑战，不仅需要稳定的视觉特征，还需要主体与环境的无缝融合。动作的合理性同样关键。无论形态和背景看起来多么真实，如果所描绘的动作在物理或逻辑上不合理，整体效果就会受到影响。<br><strong>扩展时长和精细控制</strong>。 当前大多数人体运动视频生成方法只能产生短片段，通常持续几秒钟。将其扩展到更长的视频，持续数分钟甚至数小时，仍然是一个重大挑战。未来的研究应该致力于开发能够在延长时间内保持连贯性和质量的技术。此外，当前的多模态方法，即使使用网格和深度图等信号，在特定身体部位的详细控制方面仍然困难重重。提高视频的真实感和表现力需要改进对手部和面部等复杂区域的控制。<br><strong>实时平台和成本</strong>。 虚拟人的实时流媒体需要低延迟来确保流畅自然的交互，而高延迟会破坏沟通并降低逼真的响应性。高质量的图形也需要大量带宽，带宽有限会导致视频质量降低和缓冲问题。此外，用户界面必须高度响应，提供即时反馈以保持用户参与度。虽然扩散模型[116]在人体运动视频生成中起着关键作用，但如附录G所示，它们的高计算需求使得开发更高效的模型来降低成本和提高可访问性变得必要。<br><strong>伦理问题</strong>。 创建数字人形引入了重大的伦理关切，特别是在个人数据使用和隐私方面。需要一个强有力的伦理框架来指导其开发和整合，确保生物识别数据的知情同意，并为其行为建立问责制。保护隐私和解决负面影响至关重要。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li>1.<img src="https://zhuanlan.zhihu.com/p/1947261732583089760" alt="同样是解读的精简版"></li>
<li>2.<img src="https://www.zhihu.com/people/zhi-zhe-zai-cao-mang/posts" alt="关于扩散的一部分内容介绍"></li>
<li>3.<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/2672965087">https://zhuanlan.zhihu.com/p/2672965087</a></li>
</ul>
	
		</div>
		
		<div id="current-post-cover" data-scr="/images/cover8.jpg"></div>

		<!-- relate post, comment...-->
		<div class="investment-container">
			<div class="investment-header">
				<div class="investment-title-1">
					<div class="on">相关文章</div>
					<div>评论</div>
					<div>分享</div>
				</div>
				<div class="investment-title-2">	            
					
	<span>
		<a id="totop-post-page">返回顶部</a>
		
			<a href="/2025/11/05/fsvid2vid/" title="论文阅读：One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing" rel="prev">
				&laquo;上一篇
			</a>
		
		
			<a href="/2025/10/29/%E5%9F%BA%E7%A1%80%E7%AF%87-%E5%AE%89%E8%A3%85PyTorch/" title="基础篇-安装PyTorch" rel="next">
				下一篇&raquo;
			</a>
			
	</span>


      		
				</div>	
			</div>
			
			<div class="investment-content">
				<div class="investment-content-list">
					

<div class="relate-post">
	
		<ul>
			
					<li>
						<div class="relate-post-text">
							<a class="relate-post-title" href="/2025/11/10/%E5%87%A0%E4%B8%AA%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97%E5%BA%93%E4%BB%8B%E7%BB%8D/" title="几个深度学习计算库介绍">
								几个深度学习计算库介绍			
							</a>
							<p class="relate-post-date">
								<i class="fa fa-calendar"></i>
								十一月 10日, 2025				
							</p>
							<p class="relate-post-content">
								einopseinops主要功能是张量维度的重排，功能介绍。
MXNethttps://blog.csdn.net/tanmx219/article/details/106790686

							</p>
						</div>

						<div class="relate-post-cover">
							<a href="/2025/11/10/%E5%87%A0%E4%B8%AA%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97%E5%BA%93%E4%BB%8B%E7%BB%8D/" title="几个深度学习计算库介绍">				
								
								<img class="lazy" src="/img/lazy.gif" data-src="/img/cart_cover.jpg" alt="几个深度学习计算库介绍"/>
							</a>
						</div>
					</li>												
			
					<li>
						<div class="relate-post-text">
							<a class="relate-post-title" href="/2025/11/10/FID%E6%8C%87%E6%A0%87%E4%BB%8B%E7%BB%8D/" title="FID指标介绍">
								FID指标介绍			
							</a>
							<p class="relate-post-date">
								<i class="fa fa-calendar"></i>
								十一月 10日, 2025				
							</p>
							<p class="relate-post-content">
								FID是衡量视频生成的一个重要指标
参考文献
1.https://zhouyifan.net/2024/04/04/20240221-TorchEval-FID/


							</p>
						</div>

						<div class="relate-post-cover">
							<a href="/2025/11/10/FID%E6%8C%87%E6%A0%87%E4%BB%8B%E7%BB%8D/" title="FID指标介绍">				
								
								<img class="lazy" src="/img/lazy.gif" data-src="/images/cover13.png" alt="FID指标介绍"/>
							</a>
						</div>
					</li>												
			
					<li>
						<div class="relate-post-text">
							<a class="relate-post-title" href="/2025/11/10/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E6%B3%A8%E5%86%8C%E9%97%AE%E9%A2%98/" title="注册（registration）问题">
								注册（registration）问题			
							</a>
							<p class="relate-post-date">
								<i class="fa fa-calendar"></i>
								十一月 10日, 2025				
							</p>
							<p class="relate-post-content">
								注册（registration）又称作配准，通常指的是在计算机视觉和图形处理中，将不同的数据集合配准（对齐）到同一坐标系或模板的过程，以便进行分析、比较或重建。非刚体配准（Non-Rigid Registration）指的是在处理形状...
							</p>
						</div>

						<div class="relate-post-cover">
							<a href="/2025/11/10/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E6%B3%A8%E5%86%8C%E9%97%AE%E9%A2%98/" title="注册（registration）问题">				
								
								<img class="lazy" src="/img/lazy.gif" data-src="/images/cover12.png" alt="注册（registration）问题"/>
							</a>
						</div>
					</li>												
			
					<li>
						<div class="relate-post-text">
							<a class="relate-post-title" href="/2025/11/06/liveportrait/" title="论文阅读：LivePortrait">
								论文阅读：LivePortrait			
							</a>
							<p class="relate-post-date">
								<i class="fa fa-calendar"></i>
								十一月 6日, 2025				
							</p>
							<p class="relate-post-content">
								背景
论文标题  Human Motion Video Generation: A Survey
期刊 Arxiv 2024
论文地址 https://arxiv.org/pdf/2407.03168
项目地址 https://git...
							</p>
						</div>

						<div class="relate-post-cover">
							<a href="/2025/11/06/liveportrait/" title="论文阅读：LivePortrait">				
								
								<img class="lazy" src="/img/lazy.gif" data-src="/images/cover11.png" alt="论文阅读：LivePortrait"/>
							</a>
						</div>
					</li>												
			
					<li>
						<div class="relate-post-text">
							<a class="relate-post-title" href="/2025/11/05/fsvid2vid/" title="论文阅读：One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing">
								论文阅读：One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing			
							</a>
							<p class="relate-post-date">
								<i class="fa fa-calendar"></i>
								十一月 5日, 2025				
							</p>
							<p class="relate-post-content">
								背景
论文标题  One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing
期刊 CVPR，2021
论文地址 https://arxiv.org/...
							</p>
						</div>

						<div class="relate-post-cover">
							<a href="/2025/11/05/fsvid2vid/" title="论文阅读：One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing">				
								
								<img class="lazy" src="/img/lazy.gif" data-src="/images/cover10.png" alt="论文阅读：One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing"/>
							</a>
						</div>
					</li>												
			
					<li>
						<div class="relate-post-text">
							<a class="relate-post-title" href="/2025/10/29/%E5%9F%BA%E7%A1%80%E7%AF%87-%E5%AE%89%E8%A3%85PyTorch/" title="基础篇-安装PyTorch">
								基础篇-安装PyTorch			
							</a>
							<p class="relate-post-date">
								<i class="fa fa-calendar"></i>
								十月 29日, 2025				
							</p>
							<p class="relate-post-content">
								今晚情绪较低，稍微看了会论文就不想工作了。由于安装Cuda和PyTorch是不得不品的一环，于是整理了一下之前的资料，写个笔记。
环境配置
包管理器：miniconda
python&#x3D;3.10

安装cuda理论上来说只要买...
							</p>
						</div>

						<div class="relate-post-cover">
							<a href="/2025/10/29/%E5%9F%BA%E7%A1%80%E7%AF%87-%E5%AE%89%E8%A3%85PyTorch/" title="基础篇-安装PyTorch">				
								
								<img class="lazy" src="/img/lazy.gif" data-src="/images/cover9.png" alt="基础篇-安装PyTorch"/>
							</a>
						</div>
					</li>												
			
					<li>
						<div class="relate-post-text">
							<a class="relate-post-title" href="/2025/09/15/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9A%E6%B7%B7%E5%85%83Avatar/" title="论文阅读：混元Avatar">
								论文阅读：混元Avatar			
							</a>
							<p class="relate-post-date">
								<i class="fa fa-calendar"></i>
								九月 15日, 2025				
							</p>
							<p class="relate-post-content">
								混元Avatar摘要混元Avatar是一个基于大型语言模型的多模态智能体，它能够理解和执行用户的自然语言指令，并与用户进行多轮对话。混元Avatar的核心技术包括：

多模态理解：能够理解用户的自然语言指令，并将其转换为可执行的动作。...
							</p>
						</div>

						<div class="relate-post-cover">
							<a href="/2025/09/15/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9A%E6%B7%B7%E5%85%83Avatar/" title="论文阅读：混元Avatar">				
								
								<img class="lazy" src="/img/lazy.gif" data-src="/img/cart_cover.jpg" alt="论文阅读：混元Avatar"/>
							</a>
						</div>
					</li>												
			
					<li>
						<div class="relate-post-text">
							<a class="relate-post-title" href="/2025/09/15/Hallo2/" title="论文阅读：Hallo2">
								论文阅读：Hallo2			
							</a>
							<p class="relate-post-date">
								<i class="fa fa-calendar"></i>
								九月 15日, 2025				
							</p>
							<p class="relate-post-content">
								参考文献：

https://zhuanlan.zhihu.com/p/3410949248




							</p>
						</div>

						<div class="relate-post-cover">
							<a href="/2025/09/15/Hallo2/" title="论文阅读：Hallo2">				
								
								<img class="lazy" src="/img/lazy.gif" data-src="/img/cart_cover.jpg" alt="论文阅读：Hallo2"/>
							</a>
						</div>
					</li>												
			
					<li>
						<div class="relate-post-text">
							<a class="relate-post-title" href="/2025/09/14/Moda/" title="论文阅读：MoDA">
								论文阅读：MoDA			
							</a>
							<p class="relate-post-date">
								<i class="fa fa-calendar"></i>
								九月 14日, 2025				
							</p>
							<p class="relate-post-content">
								我在8月末帮助师兄的项目时，需要做近期的一些工作汇总，期间阅读了相关的一些论文，作笔记记录。
背景介绍最新一波的人工智能生成内容（AIGC）在计算机视觉领域取得了显著的成功，扩散模型在这一成就中扮演了关键角色。由于其印象深刻的生成能力...
							</p>
						</div>

						<div class="relate-post-cover">
							<a href="/2025/09/14/Moda/" title="论文阅读：MoDA">				
								
								<img class="lazy" src="/img/lazy.gif" data-src="/images/cover6.jpg" alt="论文阅读：MoDA"/>
							</a>
						</div>
					</li>												
			
					<li>
						<div class="relate-post-text">
							<a class="relate-post-title" href="/2025/10/26/%E4%BB%A3%E7%90%86%E9%94%99%E8%AF%AF%EF%BC%88SSLEOFError%EF%BC%89%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/" title="代理错误（SSLEOFError)解决方法">
								代理错误（SSLEOFError)解决方法			
							</a>
							<p class="relate-post-date">
								<i class="fa fa-calendar"></i>
								十月 26日, 2025				
							</p>
							<p class="relate-post-content">
								最近安装包的时候突然就出现错误了,在之前学会设置powershell里的代理后基本没有碰到过此类的问题，所以还是很意外的。之后的流程也是经典地先去issue找，发现没有，上网直接搜报错类型名称，最后也是找到了一篇CSDN的帖子，解决了...
							</p>
						</div>

						<div class="relate-post-cover">
							<a href="/2025/10/26/%E4%BB%A3%E7%90%86%E9%94%99%E8%AF%AF%EF%BC%88SSLEOFError%EF%BC%89%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/" title="代理错误（SSLEOFError)解决方法">				
								
								<img class="lazy" src="/img/lazy.gif" data-src="/images/cover1.png" alt="代理错误（SSLEOFError)解决方法"/>
							</a>
						</div>
					</li>												
			
		</ul>
	
</div>	
				</div>
				<div class="investment-content-list">
					<div class="layout-comment">

	
		<div class="config-info">
			Please check the parameter of <b>comment</b> in config.yml of hexo-theme-Annie!
		</div>	
	

</div>
				</div>
				<div class="investment-content-list">
					<div class="layout-share">
	
	

		
			
			<!-- socialShare share -->
			<div class="social-share"></div>

<!--  css & js -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/social-share.js/1.0.16/css/share.min.css">
<script async src="https://cdnjs.cloudflare.com/ajax/libs/social-share.js/1.0.16/js/social-share.min.js"></script>
			
		
		
	
</div>


				</div>
			</div>	
		</div>
	</div>
</div>

<!-- show math formula -->

	<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script>
	<script type="text/x-mathjax-config">
		MathJax.Hub.Config(
			{ 
				tex2jax: {
					inlineMath: [['$','$'], ['\\(','\\)']]
				} 
			}
		);
	</script>



	 
	
<script src="/plugin/clipboard/clipboard.js"></script>

	<script>
		// Copy code !
	    function preprocessing() {
	        $("#article-content .highlight").each(function() {
	            $(this).wrap('<div id="post-code"></div>');
	        })

	        $("#article-content #post-code").each(function() {
	            $(this).prepend('<nav class="copy-nav"><span><i class="code-language"></i></span></nav>');
	        })

	        $("#article-content .copy-nav").each(function() {
	            let languageClass = $(this).next().attr('class'),
	                language = ((languageClass.length > 9) && (languageClass != null)) ? languageClass.substr(10) : "none"; //why 9? Need to check language?

	            $(this).find('.code-language').text(language);
	            $(this).append('<span class="copy-btn icon-paste"></span>');
	        });
	    }

		function copy() {
		    $('#article-content #post-code').each(function(i) {
		        let codeCopyId = 'codeCopy-' + i;

		        let codeNode = $(this).find('.code'),
		            copyButton = $(this).find('.copy-btn');

		        codeNode.attr('id', codeCopyId);
		        copyButton.attr('data-clipboard-target-id', codeCopyId);
		    })
   
			let clipboard = new ClipboardJS('.copy-btn', {
					target: function(trigger) {
						return document.getElementById(trigger.getAttribute('data-clipboard-target-id'));
					}
		      	});

			//pure js
			function showTooltip(elem, msg) {		   
				elem.setAttribute('aria-label', msg);
				elem.setAttribute('class', 'copy-btn icon-clipboard1');
				setTimeout(function() {
					elem.setAttribute('class', 'copy-btn icon-paste');
				}, 2000);
			}

			clipboard.on('success', function(e) {
			    e.clearSelection();
			    console.info('Action:', e.action);		   
			    console.info('Trigger:', e.trigger);
			    showTooltip(e.trigger, 'Copied!');   
			});
			
			clipboard.on('error', function(e) {
			    console.error('Action:', e.action);
			    console.error('Trigger:', e.trigger);
			});
		}
		
		(function copyCode(){
			if ($('.layout-post').length) {
			    preprocessing();
			    copy();
			} 
		})();
	</script>






<link rel="stylesheet" href="/plugin/fancybox/jquery.fancybox.css">


<script src="/plugin/fancybox/jquery.fancybox.js"></script>


<script type="text/javascript">
	(function gallerySet(){
		let titleID = $('.article-title a'),
			imageID = $('.article-content img'),
			videoID = $('.article-content video');
		
		let postTitle = titleID.text() ? titleID.text() : "No post title!";
		
		imageID.each(function() {
			let imgPath = $(this).attr('src'),
				imgTitle = $(this).attr('alt') ? $(this).attr('alt') : "No image description!";
		
			//给每个匹配的<img>元素打包, 即添加父元素<a>
			$(this).wrap('<a data-fancybox="gallery" data-caption="《 ' + postTitle + ' 》' + imgTitle + '"href="' + imgPath + '"> </a>');
		});
		
		videoID.each(function() {
			let videoPath = $(this).attr('src');
		
			//给每个匹配的<img>元素打包, 即添加父元素<a>
			$(this).wrap('<a data-fancybox href=" ' + videoPath + ' "> </a>');
		});
		
		//TODO：支持html5 video

		if($('#layout-post').length) {
			$('[data-fancybox="gallery"]').fancybox({
				loop: true,
				buttons: [
					"zoom",
					"share",
					"slideShow",
					"fullScreen",
					//"download",
					"thumbs",
					"close"
				],
				protect: true
			});
		}
	})();
</script>
		</main>

		<!--footer-->
		<footer>
	<div id="navigation-show">
		<ul id="global-nav">
	
		<li class="menu-home">
			<a href="/" class="menu-item-home" target="_blank">主页</a>
		</li>
		
	
		<li class="menu-archive">
			<a href="/archives" class="menu-item-archive" target="_blank">归档</a>
		</li>
		
	
		<li class="menu-categories">
			<a href="/categories" class="menu-item-categories" target="_blank">分类</a>
		</li>
		
	
		<li class="menu-tags">
			<a href="/tags" class="menu-item-tags" target="_blank">标签</a>
		</li>
		
	
		<li class="menu-about">
			<a href="/about" class="menu-item-about" target="_blank">关于</a>
		</li>
		
	
		<li class="menu-gallery">
			<a href="/gallery" class="menu-item-gallery" target="_blank">相册</a>
		</li>
		
	

	
		<li class="menu-search">
			<a href="javascript:;" class="popup-trigger">搜索</a>
		</li>
	
</ul>
	</div>

	<div class="copyright">
		<p>
			 
				&copy;2024 - 2025, content by Sextent. All Rights Reserved.
			
			
				<a href="http://hexo.io/" title="Hexo" target="_blank" rel="noopener">Hexo</a> Theme <a href="https://github.com/Sariay/hexo-theme-Annie" title="Annie" target="_blank" rel="noopener">Annie</a> by Sariay.
			
		</p>
		<p>
			

	<!-- busuanzi -->
	<!-- busuanzi -->

		
	<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
	

		<span id="busuanzi_container_page_pv">
	  		本文总阅读量<span id="busuanzi_value_page_pv"></span>次
		</span>

	




			<a href="javascript:zh_tran('s');" class="zh_click" id="zh_click_s">简体</a> 
			<a href="javascript:zh_tran('t');" class="zh_click" id="zh_click_t">繁體</a>				
		</p>
	</div>		
</footer>
		
	<!-- Local or hitokoto! -->

	
<script src="/plugin/motto/motto.js"></script>

	
	<script type="text/javascript">
		(function motto(){
			let mottoText = getMingYanContent().split('</br> - </br>'),
			
			mottoTextContent = mottoText[0]?mottoText[0]:'请刷新...',
			
			mottoTextFrom = mottoText[1]?mottoText[1]:'one/一个';
			
			mottoTextContent = mottoTextContent.trim().substring(0, 100);
		
			$("#motto-content").html( mottoTextContent);
			$("#motto-author").html( mottoTextFrom  );
		})();	
	</script>	



<!-- love effect -->

	
<script src="/plugin/love/love.js"></script>



<!-- back to top -->

	<div id="totop">
	<span class="icon-circle-up"></span>
</div>



<!-- site analysis -->


	<!-- site-analysis -->
	
	
	
	
	
 

<!-- leancloud -->


	<!-- leancloud -->
	<!--
	时间：2018-11-27
	描述：
		文章访问量：visitors
		文章喜欢量：likes	
		文章排行榜：topNPost
		其他得说明：
			01-Cookie相关的函数 
				https://blog.csdn.net/somehow1002/article/details/78511541（Author：somehow1002）
			02-visitors相关的函数 
				https://blog.csdn.net/u013553529/article/details/63357382（Author：爱博客大伯）
				https://notes.doublemine.me/2015-10-21-为NexT主题添加文章阅读量统计功能.html（Author：夏末）
			03-topNPost相关的函数
				https://hoxis.github.io/hexo-next-read-rank.html（Author：hoxis）
			04-likes相关的函数，
				参考了01 & 02进行简单的设计与实现
-->


	

  

	<!--
	时间：2018-10-3
	描述：
		插件名称：hexo-generator-search-zip
		插件来源: https://github.com/SuperKieran/hexo-generator-search-zip
		代码参考：https://github.com/SuperKieran/TKL/blob/master/layout/_partial/search.ejs(Include: js & css)	
-->
<div class="popup search-popup local-search-popup scrollbar" >
	<div class="local-search-container">
		<span class="popup-btn-close">
      		ESC
   		</span>
		<div class="local-search-header">
			<div class="input-prompt">				
			</div>
			<input autocomplete="off" placeholder="Search..." type="text" id="local-search-input">
		</div>
		<div class="local-search-body">
			<div id="local-search-output"></div>
		</div>
		<div class="local-search-footer">
			<div class="topN-post">				
				
								
			</div>
		</div>
	</div>
</div>


<script src="/plugin/search/ziploader.js"></script>
<script src="/js/search.js"></script>


<script type="text/javascript">
	var search_path = 'search.json',
		zip_Path = '/search.zip',
		version_Path = '/searchVersion.txt',
		input_Trigger = 'auto',
		top_N = '2';

	themeLocalSearch({
		search_path, 
		zip_Path, 
		version_Path, 
		input_Trigger, 
		top_N
	});
</script>



<script src="/plugin/chinese/chinese.js"></script>
<script src="/plugin/imagelazyloader/yall.min.js"></script>
<script src="/plugin/imageloaded/imagesloaded.pkgd.min.js"></script>
<script src="/plugin/nicescroll/jquery.nicescroll.js"></script>
<script src="/plugin/resizediv/resizediv.js"></script>
<script src="/js/main.js"></script>

	</body>	
</html>